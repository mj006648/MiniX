# spark-jobs/03-pipeline-schedule.yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: ScheduledSparkApplication
metadata:
  name: parking-validation-schedule
  namespace: spark-operator
spec:
  schedule: "*/35 * * * *"
  concurrencyPolicy: Forbid
  successfulRunHistoryLimit: 3
  failedRunHistoryLimit: 3
  template:
    type: Python
    mode: cluster
    image: "ich6648/spark-iceberg-nessie-kafka:9.0"
    imagePullPolicy: Always
    mainApplicationFile: "local:///mnt/scripts/data_validator.py"
    sparkVersion: "3.5.0"
    sparkConf:
      spark.jars.packages: "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"
      # ✅ Secret 값을 직접 참조하여 sparkConf에 주입합니다.
      spark.hadoop.fs.s3a.endpoint: "http://rook-ceph-rgw-my-store.rook-ceph.svc:8080" # Endpoint는 고정값이므로 직접 입력
      spark.hadoop.fs.s3a.access.key:
        valueFrom:
          secretKeyRef:
            name: s3-creds-for-spark
            key: S3_ACCESS_KEY
      spark.hadoop.fs.s3a.secret.key:
        valueFrom:
          secretKeyRef:
            name: s3-creds-for-spark
            key: S3_SECRET_KEY
    driver:
      serviceAccount: spark-sa
      cores: 1
      memory: "1g"
      # envFrom은 이제 필요 없으므로 삭제하거나 주석 처리합니다.
      # envFrom:
      #   - secretRef:
      #       name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    executor:
      instances: 2
      cores: 1
      memory: "1g"
      # envFrom은 이제 필요 없으므로 삭제하거나 주석 처리합니다.
      # envFrom:
      #   - secretRef:
      #       name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    volumes:
      - name: scripts-volume
        configMap:
          name: spark-scripts
---
apiVersion: sparkoperator.k8s.io/v1beta2
kind: ScheduledSparkApplication
metadata:
  name: parking-merge-to-main-schedule
  namespace: spark-operator
spec:
  schedule: "*/5 * * * *"
  concurrencyPolicy: Forbid
  successfulRunHistoryLimit: 3
  failedRunHistoryLimit: 3
  template:
    type: Python
    mode: cluster
    image: "ich6648/spark-iceberg-nessie-kafka:9.0"
    mainApplicationFile: "local:///mnt/scripts/merge_to_main.py"
    sparkVersion: "3.5.0"
    sparkConf:
      spark.jars.packages: "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"
      # ✅ Secret 값을 직접 참조하여 sparkConf에 주입합니다.
      spark.hadoop.fs.s3a.endpoint: "http://rook-ceph-rgw-my-store.rook-ceph.svc:8080"
      spark.hadoop.fs.s3a.access.key:
        valueFrom:
          secretKeyRef:
            name: s3-creds-for-spark
            key: S3_ACCESS_KEY
      spark.hadoop.fs.s3a.secret.key:
        valueFrom:
          secretKeyRef:
            name: s3-creds-for-spark
            key: S3_SECRET_KEY
    driver:
      serviceAccount: spark-sa
      cores: 1
      memory: "1g"
      # envFrom은 이제 필요 없으므로 삭제하거나 주석 처리합니다.
      # envFrom:
      #   - secretRef:
      #       name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    executor:
      instances: 1
      cores: 1
      memory: "1g"
      # envFrom은 이제 필요 없으므로 삭제하거나 주석 처리합니다.
      # envFrom:
      #   - secretRef:
      #       name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    volumes:
      - name: scripts-volume
        configMap:
          name: spark-scripts
