# 1. 모델 설정 (Llama 3.1 8B)
ollama:
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1
  models:
    pull:
      - "llama3.1:8b" # 가장 표준적이고 안정적인 8B 모델
      - "bge-m3"      # 임베딩용 (필요시 유지)

# 2. 노드 선택 (GPU 노드로 고정)
nodeSelector:
  nvidia.com/gpu.present: "true"

# 3. 네트워크 이슈 해결 (중요!)
# 통신 문제를 피하기 위해 호스트 네트워크를 사용합니다.
hostNetwork: true
dnsPolicy: ClusterFirstWithHostNet

# 4. 리소스 할당 (8B 모델은 자원이 여유로우므로 안정성에 집중)
resources:
  requests:
    cpu: "2"
    memory: "8Gi"
    nvidia.com/gpu: 1
  limits:
    cpu: "4"
    memory: "16Gi"
    nvidia.com/gpu: 1

# 5. 스토리지 설정
persistentVolume:
  enabled: true
  storageClass: "rook-ceph-block"
  size: 50Gi # 8B 모델은 5GB 내외라 50GB면 아주 넉넉합니다.
