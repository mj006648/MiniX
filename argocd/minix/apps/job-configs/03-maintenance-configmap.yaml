# FILE: argocd/minix/apps/job-configs/03-maintenance-configmap.yaml
# ìµœì¢… ë²„ì „: ìŠ¤ëƒ…ìƒ· ë§Œë£Œ(expire_snapshots) ë¡œì§ì„ ì œê±°í•˜ì—¬ ë°ì´í„°ì˜ ì˜êµ¬ ë³´ì¡´ì„ ë³´ì¥

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-maintenance-scripts
  namespace: spark-operator
data:
  # --- SCRIPT: Iceberg í…Œì´ë¸” ìµœì í™” ë° ìœ ì§€ë³´ìˆ˜ (ìŠ¤ëƒ…ìƒ· ë§Œë£Œ ì œì™¸) ---
  daily_table_maintenance.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, trim, lower
    from datetime import datetime, timedelta

    S3_ENDPOINT = os.environ.get("S3_ENDPOINT")
    S3_ACCESS_KEY = os.environ.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.environ.get("S3_SECRET_KEY")
    NESSIE_URI = "http://nessie.nessie-ns.svc:19120/api/v1"
    WAREHOUSE_PATH = "s3a://iceberg-data/warehouse"

    def get_spark_session():
        builder = (
            SparkSession.builder.appName("TableMaintenance") # ì•± ì´ë¦„ ë³€ê²½
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.warehouse", WAREHOUSE_PATH)
            .config("spark.sql.catalog.nessie.ref", "main")
        )
        if S3_ENDPOINT: builder.config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT)
        if S3_ACCESS_KEY: builder.config("spark.hadoop.fs.s3a.access.key", S3_ACCESS_KEY)
        if S3_SECRET_KEY: builder.config("spark.hadoop.fs.s3a.secret.key", S3_SECRET_KEY)
        builder.config("spark.hadoop.fs.s3a.path.style.access", "true")
        builder.config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        builder.config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        return builder.getOrCreate()

    def get_all_prod_tables(spark):
        try:
            namespaces_df = spark.sql("SHOW NAMESPACES IN nessie")
            prod_namespaces = [row.namespace for row in namespaces_df.filter(col("namespace").rlike(r".*_prod$")).collect()]
            
            all_tables = []
            for ns in prod_namespaces:
                tables_df = spark.sql(f"SHOW TABLES IN nessie.`{ns}`")
                for row in tables_df.collect():
                    all_tables.append(f"nessie.`{ns}`.`{row.tableName}`")
            return all_tables
        except Exception as e:
            print(f"ğŸ”¥ ERROR: Could not list production tables. Reason: {e}")
            return []

    def main():
        spark = get_spark_session()
        print("--- Starting Table Maintenance Job ---")

        prod_tables = get_all_prod_tables(spark)
        if not prod_tables:
            print("No production tables found to maintain. Exiting.")
            spark.stop()
            return

        print(f"Found {len(prod_tables)} production tables to maintain: {prod_tables}")

        older_than_timestamp = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d %H:%M:%S')

        for table in prod_tables:
            print(f"\n--- Maintaining table: {table} ---")
            try:
                # 1. ê³ ì•„ íŒŒì¼ ì œê±°
                print(f"1. Removing orphan files older than {older_than_timestamp}...")
                spark.sql(f"CALL nessie.system.remove_orphan_files(table => '{table}', older_than => TIMESTAMP '{older_than_timestamp}', dry_run => false)")
                print("âœ… Orphan files removed.")

                # 2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìµœì í™”
                print("2. Rewriting manifests...")
                spark.sql(f"CALL nessie.system.rewrite_manifests(table => '{table}', use_caching => false)")
                print("âœ… Manifests rewritten.")

                # 3. ë°ì´í„° íŒŒì¼ ì••ì¶• (Compaction)
                print("3. Rewriting data files (compaction) with sort strategy...")
                spark.sql(f"""
                    CALL nessie.system.rewrite_data_files(
                        table => '{table}',
                        strategy => 'sort',
                        sort_order => 'timestamp DESC NULLS LAST'
                    )
                """)
                print("âœ… Data files compacted.")

                # --- 4. ìŠ¤ëƒ…ìƒ· ë§Œë£Œ ë¡œì§ì€ ì—¬ê¸°ì„œ ì œê±°ë¨ ---

            except Exception as e:
                print(f"ğŸ”¥ ERROR: Failed to maintain table {table}. Reason: {e}")
                continue

        print("\n--- Table Maintenance Job Finished ---")
        spark.stop()

    if __name__ == "__main__":
        main()
