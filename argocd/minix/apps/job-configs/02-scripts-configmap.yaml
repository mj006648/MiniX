# FILE: argocd/minix/apps/job-configs/02-scripts-configmap.yaml
# 최종 수정: 잘못된 Iceberg 함수 호출 수정 및 제안된 논리 흐름 완벽 반영

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-scripts
  namespace: spark-operator
data:
  dynamic_all_in_one_processor.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import from_json, col, lit, struct, to_json, current_timestamp, when
    from pyspark.sql.utils import AnalysisException

    # --- 1. 환경 변수 및 상수 정의 ---
    S3_ENDPOINT = os.environ.get("S3_ENDPOINT")
    S3_ACCESS_KEY = os.environ.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.environ.get("S3_SECRET_KEY")
    NESSIE_URI = "http://nessie.nessie-ns.svc:19120/api/v1"
    KAFKA_BOOTSTRAP_SERVERS = "10.34.48.245:9092"
    WAREHOUSE_PATH = "s3a://iceberg-data/warehouse"

    def get_spark_session():
        """Spark 세션을 초기화하고 반환합니다."""
        return (
            SparkSession.builder.appName("DynamicAllInOneProcessor")
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.warehouse", WAREHOUSE_PATH)
            .getOrCreate()
        )

    def process_and_publish(spark, micro_batch_df):
        """각 마이크로 배치를 처리하고 발행하는 핵심 함수입니다."""
        if micro_batch_df.rdd.isEmpty():
            return

        distinct_topics = [row.topic for row in micro_batch_df.select("topic").distinct().collect()]
        for topic_name in distinct_topics:
            print(f"\n--- Processing topic: {topic_name} ---")
            topic_df = micro_batch_df.filter(col("topic") == topic_name)
            
            clean_topic_name = topic_name.replace('-', '_')
            prod_namespace = f"nessie.{clean_topic_name}_prod"
            staging_namespace = f"nessie.{clean_topic_name}_staging"
            trusted_table = f"{clean_topic_name}_trusted"
            untrusted_table = f"{clean_topic_name}_untrusted"
            
            # --- 2. 최초 온보딩 로직 (사용자 제안 로직 완벽 반영) ---
            spark.sql("USE REFERENCE main IN nessie")

            # 네임스페이스 생성
            spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {prod_namespace}")
            spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {staging_namespace}")
            
            # 테이블 존재 여부 확인 후 생성
            if not spark.catalog.tableExists(f"{prod_namespace}.{trusted_table}"):
                print(f"Tables for topic '{topic_name}' not found. Creating...")
                json_rdd = topic_df.select(col("value").cast("string")).rdd.map(lambda r: r.value)
                if not json_rdd.isEmpty():
                    json_df = spark.read.json(json_rdd)
                    # --- [핵심 수정] 올바른 Iceberg 유틸리티 함수 사용 ---
                    schema_ddl = spark.sparkContext._jvm.org.apache.iceberg.spark.SparkSQLUtil.schemaToDDL(json_df.schema)
                    partition_by = "PARTITIONED BY (days(timestamp))" if "timestamp" in json_df.columns else ""

                    spark.sql(f"CREATE TABLE {prod_namespace}.{trusted_table} ({schema_ddl}) USING iceberg {partition_by}")
                    spark.sql(f"CREATE TABLE {staging_namespace}.{trusted_table} ({schema_ddl}) USING iceberg {partition_by}")
                    spark.sql(f"CREATE TABLE {staging_namespace}.{untrusted_table} (raw_payload STRING, validation_error STRING, processing_timestamp TIMESTAMP) USING iceberg PARTITIONED BY (days(processing_timestamp))")
                    print(f"Table creation for topic '{topic_name}' complete.")

            # dev 브랜치 존재 여부 확인 및 생성
            try:
                spark.sql("USE REFERENCE dev IN nessie")
            except Exception:
                print("Branch 'dev' not found. Creating it from 'main'...")
                spark.sql("USE REFERENCE main IN nessie")
                spark.sql("CREATE BRANCH dev IN nessie FROM main")

            # --- 3. 데이터 처리 및 발행 ---
            spark.sql("USE REFERENCE dev IN nessie")
            json_rdd = topic_df.select(col("value").cast("string")).rdd.map(lambda r: r.value)
            if json_rdd.isEmpty(): continue
            json_df = spark.read.json(json_rdd)
            
            validated_df = json_df.withColumn("validation_error", when(col("eventId").isNull(), "eventId is null").otherwise(lit(None)))
            trusted_df = validated_df.filter(col("validation_error").isNull())
            untrusted_df = validated_df.filter(col("validation_error").isNotNull())

            # dev 브랜치에 커밋
            if not trusted_df.rdd.isEmpty():
                trusted_df.writeTo(f"{staging_namespace}.{trusted_table}").append()
                print(f"Committed data for '{topic_name}' to 'dev' branch.")
            
            if not untrusted_df.rdd.isEmpty():
                untrusted_df_with_payload = untrusted_df.withColumn("raw_payload", to_json(struct([col(c) for c in untrusted_df.columns])))
                untrusted_df_with_payload.select("raw_payload", "validation_error").withColumn("processing_timestamp", current_timestamp()).writeTo(f"{staging_namespace}.{untrusted_table}").append()

            # main 브랜치로 발행
            if not trusted_df.rdd.isEmpty():
                print(f"Publishing trusted data for '{topic_name}' to 'main' branch...")
                trusted_df.createOrReplaceTempView("source_view")
                spark.sql(f"MERGE INTO {prod_namespace}.{trusted_table} t USING source_view s ON t.eventId = s.eventId WHEN NOT MATCHED THEN INSERT *")
                print(f"Successfully published trusted data for '{topic_name}'.")

    def main():
        spark = get_spark_session()
        spark.sparkContext.setLogLevel("WARN")

        kafka_df = (
            spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
            .option("subscribePattern", "parking-.*")
            .option("startingOffsets", "earliest")
            .option("failOnDataLoss", "false")
            .load()
        )
        
        query = (
            kafka_df.writeStream
            .foreachBatch(lambda df, batch_id: process_and_publish(spark, df))
            .option("checkpointLocation", f"{WAREHOUSE_PATH}/_checkpoints/all_in_one_processor")
            .trigger(processingTime='5 minutes')
            .start()
        )
        query.awaitTermination()

    if __name__ == "__main__":
        main()
