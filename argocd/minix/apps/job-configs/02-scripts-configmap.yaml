# FILE: argocd/minix/apps/job-configs/02-scripts-configmap.yaml
# 최종 수정: parking-.* 패턴, 5분 주기, 브랜치 컨텍스트, import, failOnDataLoss 문제 모두 해결

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-scripts
  namespace: spark-operator
data:
  dynamic_all_in_one_processor.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import from_json, col, lit, struct, to_json, current_timestamp, when
    from pyspark.sql.utils import AnalysisException

    S3_ENDPOINT = os.environ.get("S3_ENDPOINT")
    S3_ACCESS_KEY = os.environ.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.environ.get("S3_SECRET_KEY")
    NESSIE_URI = "http://nessie.nessie-ns.svc:19120/api/v1"
    KAFKA_BOOTSTRAP_SERVERS = "10.34.48.245:9092"
    WAREHOUSE_PATH = "s3a://iceberg-data/warehouse"

    def get_spark_session():
        return (
            SparkSession.builder.appName("DynamicAllInOneProcessor")
            .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.ref", "dev")
            .config("spark.sql.catalog.nessie.warehouse", WAREHOUSE_PATH)
            .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT)
            .config("spark.hadoop.fs.s3a.access.key", S3_ACCESS_KEY)
            .config("spark.hadoop.fs.s3a.secret.key", S3_SECRET_KEY)
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .getOrCreate()
        )

    def process_and_publish(spark, micro_batch_df):
        if micro_batch_df.rdd.isEmpty():
            return

        distinct_topics = [row.topic for row in micro_batch_df.select("topic").distinct().collect()]
        for topic_name in distinct_topics:
            print(f"--- Processing topic: {topic_name} ---")
            topic_df = micro_batch_df.filter(col("topic") == topic_name)
            
            clean_topic_name = topic_name.replace('-', '_')
            prod_namespace = f"nessie.{clean_topic_name}_prod"
            staging_namespace = f"nessie.{clean_topic_name}_staging"
            trusted_table = f"{clean_topic_name}_trusted"
            untrusted_table = f"{clean_topic_name}_untrusted"
            
            try:
                spark.sql("USE REFERENCE main IN nessie")
                spark.sql(f"SHOW TABLES IN {prod_namespace}")
            except AnalysisException:
                print(f"First message for topic '{topic_name}'. Starting Onboarding...")
                spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {prod_namespace}")
                spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {staging_namespace}")
                
                json_rdd = topic_df.select(col("value").cast("string")).rdd.map(lambda r: r.value)
                if json_rdd.isEmpty(): continue
                json_df = spark.read.json(json_rdd)
                schema_ddl = spark.sparkContext._jvm.org.apache.iceberg.spark.SparkSchemaUtil.schemaToDDL(json_df.schema)
                partition_by = "PARTITIONED BY (days(timestamp))" if "timestamp" in json_df.columns else ""

                spark.sql(f"CREATE TABLE {prod_namespace}.{trusted_table} ({schema_ddl}) USING iceberg {partition_by}")
                spark.sql(f"CREATE TABLE {staging_namespace}.{trusted_table} ({schema_ddl}) USING iceberg {partition_by}")
                spark.sql(f"CREATE TABLE {staging_namespace}.{untrusted_table} (raw_payload STRING, validation_error STRING, processing_timestamp TIMESTAMP) USING iceberg PARTITIONED BY (days(processing_timestamp))")
                
                try:
                    spark.sql("CREATE BRANCH dev IN nessie FROM main")
                except Exception: pass
                print(f"Onboarding for topic '{topic_name}' complete.")
            
            spark.sql("USE REFERENCE dev IN nessie")
            json_df = spark.read.json(topic_df.select(col("value").cast("string")).rdd.map(lambda r: r.value))
            
            validated_df = json_df.withColumn("validation_error", when(col("eventId").isNull(), "eventId is null").otherwise(lit(None)))
            trusted_df = validated_df.filter(col("validation_error").isNull())
            untrusted_df = validated_df.filter(col("validation_error").isNotNull())

            if not trusted_df.rdd.isEmpty():
                trusted_df.select(json_df.columns).writeTo(f"{staging_namespace}.{trusted_table}").append()
            
            if not untrusted_df.rdd.isEmpty():
                untrusted_df_with_payload = untrusted_df.withColumn("raw_payload", to_json(struct([col(c) for c in untrusted_df.columns])))
                untrusted_df_with_payload.select("raw_payload", "validation_error").withColumn("processing_timestamp", current_timestamp()).writeTo(f"{staging_namespace}.{untrusted_table}").append()
            
            print(f"Committed data for '{topic_name}' to 'dev' branch.")

            if not trusted_df.rdd.isEmpty():
                print(f"Publishing trusted data for '{topic_name}' to 'main' branch...")
                try:
                    spark.sql(f"MERGE INTO {prod_namespace}.{trusted_table} AS t USING (SELECT * FROM {staging_namespace}.{trusted_table}@dev AS s) ON t.eventId = s.eventId WHEN NOT MATCHED THEN INSERT *")
                    print(f"Successfully published trusted data for '{topic_name}'.")
                except Exception as e:
                    print(f"Error publishing data for topic '{topic_name}': {e}")

    def main():
        spark = get_spark_session()
        spark.sparkContext.setLogLevel("WARN")

        kafka_df = (
            spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
            .option("subscribePattern", "parking-.*")
            .option("startingOffsets", "earliest")
            .option("failOnDataLoss", "false")
            .load()
        )
        
        query = (
            kafka_df.writeStream
            .foreachBatch(lambda df, batch_id: process_and_publish(spark, df))
            .option("checkpointLocation", f"{WAREHOUSE_PATH}/_checkpoints/all_in_one_processor")
            .trigger(processingTime='5 minutes')
            .start()
        )
        query.awaitTermination()

    if __name__ == "__main__":
        main()
