# spark-jobs/02-pipeline-schedule.yaml
# --- 작업 1: 데이터 품질 검증 (매시간 0분에 실행) ---
apiVersion: sparkoperator.k8s.io/v1beta2
kind: ScheduledSparkApplication
metadata:
  name: parking-validation-schedule
  namespace: spark-operator
spec:
  schedule: "0 * * * *"
  concurrencyPolicy: Forbid
  successfulRunHistoryLimit: 3
  failedRunHistoryLimit: 3
  template:
    type: Python
    mode: cluster
    image: "ich6648/spark-iceberg-nessie-kafka:9.0"
    imagePullPolicy: Always
    mainApplicationFile: "local:///mnt/scripts/data_validator.py"
    sparkVersion: "3.5.0"
    driver:
      serviceAccount: spark-sa
      cores: 1
      memory: "1g"
      envFrom:
        - secretRef:
            name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    executor:
      instances: 2
      cores: 1
      memory: "1g"
      envFrom:
        - secretRef:
            name: s3-creds-for-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    volumes:
      - name: scripts-volume
        configMap:
          name: spark-scripts
---
# --- 작업 2: Main 브랜치 병합 (매시간 10분에 실행) ---
apiVersion: sparkoperator.k8s.io/v1beta2
kind: ScheduledSparkApplication
metadata:
  name: parking-merge-to-main-schedule
  namespace: spark-operator
spec:
  schedule: "10 * * * *"
  concurrencyPolicy: Forbid
  successfulRunHistoryLimit: 3
  failedRunHistoryLimit: 3
  template:
    type: Python
    mode: cluster
    image: "ich6648/spark-iceberg-nessie-kafka:9.0"
    mainApplicationFile: "local:///mnt/scripts/merge_to_main.py"
    sparkVersion: "3.5.0"
    driver:
      serviceAccount: spark-sa
      cores: 1
      memory: "512m"
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    executor:
      # [수정] Spark Operator 유효성 검사 규칙에 따라 instances는 1 이상이어야 합니다.
      instances: 1
      cores: 1
      memory: "512m"
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    volumes:
      - name: scripts-volume
        configMap:
          name: spark-scripts
# --- 3. PySpark 스크립트가 담긴 ConfigMap ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-scripts
  namespace: spark-operator
data:
  # 첫 번째 스크립트: data_validator.py (staging 네임스페이스 사용 버전)
  data_validator.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, lit, when, row_number, to_json, struct, current_timestamp
    from pyspark.sql.window import Window
    from pyspark.sql.utils import AnalysisException

    S3_ENDPOINT = os.environ.get("S3_ENDPOINT")
    S3_ACCESS_KEY = os.environ.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.environ.get("S3_SECRET_KEY")
    NESSIE_URI = "http://10.34.48.249:19120/api/v1"

    def get_spark_session():
        return (
            SparkSession.builder.appName("ParkingDataValidation")
            .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.ref", "dev") # 모든 작업은 'dev' 브랜치에서 수행
            .config("spark.sql.catalog.nessie.warehouse", "s3a://iceberg/warehouse")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT)
            .config("spark.hadoop.fs.s3a.access.key", S3_ACCESS_KEY)
            .config("spark.hadoop.fs.s3a.secret.key", S3_SECRET_KEY)
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .getOrCreate()
        )

    def process_data(spark, data_path, event_type):
        try:
            raw_df = spark.read.json(data_path)
            if raw_df.rdd.isEmpty():
                print(f"No new data found in {data_path}. Skipping.")
                return
        except AnalysisException as e:
            if "Path does not exist" in str(e):
                print(f"Path {data_path} does not exist. Skipping.")
                return
            raise e

        df_with_payload = raw_df.withColumn("raw_payload", to_json(struct([col(c) for c in raw_df.columns])))
        window_spec = Window.partitionBy("eventId").orderBy(col("timestamp").desc())
        df_with_row_num = df_with_payload.withColumn("row_num", row_number().over(window_spec))

        validated_df = df_with_row_num.withColumn(
            "validation_error",
            when(col("eventId").isNull(), "Primary key is null")
            .when(col("row_num") > 1, "Duplicate primary key")
            .when(
                (col("eventType") == "EXIT") & (col("eventDetails.durationInSeconds") < 0),
                "Negative duration"
            )
            .otherwise(lit(None))
        )

        trusted_df = validated_df.filter(col("validation_error").isNull())
        untrusted_df = validated_df.filter(col("validation_error").isNotNull())

        if not trusted_df.rdd.isEmpty():
            if event_type == "ENTRY":
                # [수정] Staging 네임스페이스의 trusted 테이블을 대상으로 함
                final_trusted_df = trusted_df.selectExpr("eventId", "timestamp", "carId", "eventDetails.imagePath as imagePath", "eventDetails.assignedSlotId as slotId", "eventDetails.slotCoordinates as slotCoordinates")
                trusted_table = "nessie.parking_data_staging.trusted_entry_events"
                final_trusted_df.writeTo(trusted_table).append()
                print(f"Appended {final_trusted_df.count()} records to {trusted_table}")
            elif event_type == "EXIT":
                final_trusted_df = trusted_df.selectExpr("eventId", "timestamp", "carId", "eventDetails.imagePath as imagePath", "eventDetails.assignedSlotId as slotId", "eventDetails.entryTimestamp as entryTimestamp", "eventDetails.durationInSeconds as durationInSeconds", "eventDetails.entryEventId as entryEventId")
                trusted_table = "nessie.parking_data_staging.trusted_exit_events"
                final_trusted_df.writeTo(trusted_table).append()
                print(f"Appended {final_trusted_df.count()} records to {trusted_table}")

        if not untrusted_df.rdd.isEmpty():
            final_untrusted_df = untrusted_df.withColumn("processing_timestamp", current_timestamp())
            if event_type == "ENTRY":
                # [수정] Staging 네임스페이스의 untrusted 테이블을 대상으로 함
                final_untrusted_df = final_untrusted_df.selectExpr("eventId", "timestamp", "carId", "eventDetails.imagePath as imagePath", "eventDetails.assignedSlotId as slotId", "eventDetails.slotCoordinates as slotCoordinates", "raw_payload", "validation_error", "processing_timestamp")
                untrusted_table = "nessie.parking_data_staging.untrusted_entry_events"
                final_untrusted_df.writeTo(untrusted_table).append()
                print(f"Appended {final_untrusted_df.count()} records to {untrusted_table}")
            elif event_type == "EXIT":
                final_untrusted_df = final_untrusted_df.selectExpr("eventId", "timestamp", "carId", "eventDetails.imagePath as imagePath", "eventDetails.assignedSlotId as slotId", "eventDetails.entryTimestamp as entryTimestamp", "eventDetails.durationInSeconds as durationInSeconds", "eventDetails.entryEventId as entryEventId", "raw_payload", "validation_error", "processing_timestamp")
                untrusted_table = "nessie.parking_data_staging.untrusted_exit_events"
                final_untrusted_df.writeTo(untrusted_table).append()
                print(f"Appended {final_untrusted_df.count()} records to {untrusted_table}")

    def main():
        spark = get_spark_session()
        from datetime import datetime, timedelta
        today = datetime.utcnow()
        yesterday = today - timedelta(days=1)
        date_paths = [ f"year={d.year}/month={d.month:02d}/day={d.day:02d}" for d in [yesterday, today] ]
        for date_path in date_paths:
            entry_path = f"s3a://iceberg/landing/parking-entries/{date_path}/*"
            process_data(spark, entry_path, "ENTRY")
            exit_path = f"s3a://iceberg/landing/parking-exits/{date_path}/*"
            process_data(spark, exit_path, "EXIT")
        spark.stop()

    if __name__ == "__main__":
        main()

  # 두 번째 스크립트: merge_to_main.py (MERGE INTO 사용 버전)
  merge_to_main.py: |
    import os
    from pyspark.sql import SparkSession
    NESSIE_URI = "http://10.34.48.249:19120/api/v1"
    def main():
        spark = (
            SparkSession.builder.appName("NessieMergeToMain")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.ref", "main")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .getOrCreate()
        )
        print("Merging trusted tables from staging@dev to prod@main...")
        try:
            # [수정] MERGE INTO 구문을 사용하여 staging의 trusted 데이터만 prod로 병합합니다.
            spark.sql("""
            MERGE INTO nessie.parking_data_prod.trusted_entry_events t
            USING (SELECT * FROM nessie.parking_data_staging.trusted_entry_events AT BRANCH dev) s
            ON t.eventId = s.eventId
            WHEN NOT MATCHED THEN INSERT *
            """)
            
            spark.sql("""
            MERGE INTO nessie.parking_data_prod.trusted_exit_events t
            USING (SELECT * FROM nessie.parking_data_staging.trusted_exit_events AT BRANCH dev) s
            ON t.eventId = s.eventId
            WHEN NOT MATCHED THEN INSERT *
            """)
            print("✅ Successfully merged trusted data.")
        except Exception as e:
            print(f"An error occurred during merge: {e}")
        spark.stop()
    if __name__ == "__main__":
        main()
