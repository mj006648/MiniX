# spark-jobs/01-job-setup.yaml

# --- 1. RBAC(역할 기반 접근 제어) 설정 ---
# Spark 드라이버 Pod가 사용할 "신분증"인 서비스 계정
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-sa
  namespace: spark-operator
---
# "익스큐터 Pod와 서비스를 생성/삭제할 수 있는 권한 목록"인 역할
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-driver-role
  namespace: spark-operator
rules:
  - apiGroups: [""]
    resources:
      - pods
      - services
      - configmaps
    # [수정] verbs에 '*'를 사용하여 모든 권한을 부여합니다.
    # 이렇게 하면 생성(create), 조회(get), 삭제(delete) 등이 모두 포함되어
    # "cannot deletecollection" 에러가 발생하지 않습니다.
    verbs: ["*"]
---
# 위에서 만든 "신분증(spark-sa)"과 "권한(spark-driver-role)"을 연결
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-driver-binding
  namespace: spark-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: spark-driver-role
subjects:
  - kind: ServiceAccount
    name: spark-sa
    namespace: spark-operator
---
# --- 2. S3 접근 정보 Secret ---
# Spark 코드가 S3(Rook-Ceph) 버킷에 안전하게 접근할 수 있도록 접속 정보를 Secret(비밀 금고)에 저장
apiVersion: v1
kind: Secret
metadata:
  name: s3-creds-for-spark
  namespace: spark-operator
type: Opaque
stringData:
  # 제공된 Rook-Ceph 버킷 정보
  S3_ENDPOINT: "http://rook-ceph-rgw-my-store.rook-ceph.svc:8080"
  S3_ACCESS_KEY: "LDCMT684CGVXV6P4E0HA"
  S3_SECRET_KEY: "E9dfgciqiyWMD0ukBXT638pY5CggcP7Ble7eBdeu"
---
# --- 3. PySpark 스크립트가 담긴 ConfigMap ---
# 실행할 두 개의 PySpark 스크립트를 쿠버네티스 ConfigMap에 "작업 설명서"처럼 저장합니다.
# 이렇게 하면 Docker 이미지를 다시 빌드하지 않고도 스크립트만 수정하고 재배포할 수 있어 편리합니다.
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-scripts
  namespace: spark-operator
data:
  # 첫 번째 스크립트: data_validator.py
  data_validator.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, lit, when, row_number
    from pyspark.sql.window import Window
    from pyspark.sql.utils import AnalysisException

    # Secret을 통해 주입될 환경 변수
    S3_ENDPOINT = os.environ.get("S3_ENDPOINT")
    S3_ACCESS_KEY = os.environ.get("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.environ.get("S3_SECRET_KEY")
    # 제공된 Nessie API Loadbalancer 주소
    NESSIE_URI = "http://10.34.48.249:19120/api/v1"

    def get_spark_session():
        return (
            SparkSession.builder.appName("ParkingDataValidation")
            .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
            .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.ref", "dev") # [핵심] 모든 작업은 'dev' 브랜치에서 수행
            .config("spark.sql.catalog.nessie.warehouse", "s3a://iceberg/warehouse")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT)
            .config("spark.hadoop.fs.s3a.access.key", S3_ACCESS_KEY)
            .config("spark.hadoop.fs.s3a.secret.key", S3_SECRET_KEY)
            .config("spark.hadoop.fs.s3a.path.style.access", "true")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            .getOrCreate()
        )

    def validate_data(df, primary_key="eventId"):
        """데이터 품질 검증(Null, 중복, 범위)을 수행하고 데이터를 분리합니다."""
        window_spec = Window.partitionBy(primary_key).orderBy(col("timestamp").desc())
        df_with_row_num = df.withColumn("row_num", row_number().over(window_spec))
        
        # [수정] eventType에 따라 검증 규칙을 다르게 적용하여 'FIELD_NOT_FOUND' 에러를 해결합니다.
        validated_df = df_with_row_num.withColumn(
            "untrusted_reason",
            when(col(primary_key).isNull(), "Primary key is null")
            .when(col("row_num") > 1, "Duplicate primary key")
            # 'durationInSeconds' 검증은 'eventType'이 'EXIT'일 때만 수행합니다.
            .when(
                (col("eventType") == "EXIT") & (col("eventDetails.durationInSeconds") < 0),
                "Negative duration"
            )
            .otherwise(lit(None))
        )
        
        trusted_df = validated_df.filter(col("untrusted_reason").isNull()).drop("row_num", "untrusted_reason")
        untrusted_df = validated_df.filter(col("untrusted_reason").isNotNull()).drop("row_num")
        return trusted_df, untrusted_df

    def process_data(spark, data_path, primary_key, trusted_table, untrusted_table):
        try:
            df = spark.read.json(data_path)
            if df.rdd.isEmpty():
                print(f"No new data found in {data_path}. Skipping.")
                return
        except AnalysisException as e:
            if "Path does not exist" in str(e):
                print(f"Path {data_path} does not exist. Skipping.")
                return
            raise e
        trusted_df, untrusted_df = validate_data(df, primary_key)
        if not trusted_df.rdd.isEmpty():
            trusted_df.writeTo(trusted_table).append()
            print(f"Appended {trusted_df.count()} records to {trusted_table}")
        if not untrusted_df.rdd.isEmpty():
            untrusted_df.writeTo(untrusted_table).append()
            print(f"Appended {untrusted_df.count()} records to {untrusted_table}")

    def main():
        spark = get_spark_session()
        from datetime import datetime, timedelta
        today = datetime.utcnow()
        yesterday = today - timedelta(days=1)
        date_paths = [ f"year={d.year}/month={d.month:02d}/day={d.day:02d}" for d in [yesterday, today] ]
        for date_path in date_paths:
            entry_path = f"s3a://iceberg/landing/parking-entries/{date_path}/*"
            process_data(spark, entry_path, "eventId", "nessie.parking_data.trusted_entry_events", "nessie.parking_data.untrusted_entry_events")
            exit_path = f"s3a://iceberg/landing/parking-exits/{date_path}/*"
            process_data(spark, exit_path, "eventId", "nessie.parking_data.trusted_exit_events", "nessie.parking_data.untrusted_exit_events")
        spark.stop()

    if __name__ == "__main__":
        main()

  # 두 번째 스크립트: merge_to_main.py
  merge_to_main.py: |
    import os
    from pyspark.sql import SparkSession

    NESSIE_URI = "http://10.34.48.249:19120/api/v1"

    def main():
        spark = (
            SparkSession.builder.appName("NessieMergeToMain")
            .config("spark.sql.catalog.nessie.uri", NESSIE_URI)
            .config("spark.sql.catalog.nessie.ref", "main")
            .config("spark.sql.extensions", "org.projectnessie.spark.extensions.NessieSparkSessionExtensions")
            .getOrCreate()
        )
        print("Attempting to merge 'dev' branch into 'main'...")
        try:
            spark.sql("MERGE BRANCH dev INTO main IN nessie")
            print("Successfully merged 'dev' into 'main'.")
        except Exception as e:
            print(f"An error occurred during merge: {e}")
        spark.stop()

    if __name__ == "__main__":
        main()
