# spark-jobs/01-job-setup.yaml

# --- RBAC 설정 ---
# Spark 드라이버 Pod가 사용할 서비스 계정
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-sa
  namespace: spark-operator
---
# 위 서비스 계정에 부여할 권한 (Pod, Service 생성/삭제 등)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-driver-role
  namespace: spark-operator
rules:
  - apiGroups: [""]
    resources: ["pods", "services", "configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
---
# 서비스 계정과 역할을 연결
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-driver-binding
  namespace: spark-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: spark-driver-role
subjects:
  - kind: ServiceAccount
    name: spark-sa
    namespace: spark-operator
---
# --- S3 접근 정보 Secret ---
apiVersion: v1
kind: Secret
metadata:
  name: s3-creds-for-spark
  namespace: spark-operator
type: Opaque
stringData:
  # 환경변수 이름은 PySpark 스크립트에서 참조할 이름과 일치해야 합니다.
  S3_ENDPOINT: "http://rook-ceph-rgw-my-store.rook-ceph.svc:8080"
  S3_ACCESS_KEY: "LDCMT684CGVXV6P4E0HA"
  S3_SECRET_KEY: "E9dfgciqiyWMD0ukBXT638pY5CggcP7Ble7eBdeu"
